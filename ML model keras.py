# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lwX8I_3iTMv5iG5vwmgw5onXKhiqbyqA
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

"""## Introduction to Machine learning with Keras and TensorFlow

**Based on examples of Daniel Moser (UT Southwestern Medical Center)**

**Resources: [Xavier Snelgrove](https://github.com/wxs/keras-mnist-tutorial), [Yash Katariya](https://github.com/yashk2810/MNIST-Keras)**

To help you understand the basics of machine learning, this demo will walk you through the basic steps of building a toy model to classify handwritten numbers with greater than 95% accuracy.

## Task for AI

Our goal is to build and train a neural network on thousands of images of handwritten digits so that it can successfully identify any digits. The data to be used is the MNIST database which contains 60,000 training images and 10,000 test images. We will be using the Keras Python API with TensorFlow.

<img src="https://github.com/AviatorMoser/keras-mnist-tutorial/blob/master/mnist.png?raw=1" >

## Import required libraries

First, some libraries need to be loaded into the Python environment.
"""

import numpy as np                   # advanced math library
import matplotlib.pyplot as plt      # MATLAB like plotting routines
import random                        # for generating random numbers

from keras.datasets import mnist     # MNIST dataset is included in Keras
from keras.models import Sequential  # Model type to be used

from keras.layers.core import Dense, Dropout, Activation # Types of layers to be used in our model
from keras.utils import np_utils                         # NumPy related tools

"""## Load the training set

The MNIST dataset is conveniently linked to Keras and we can easily parse some of its functionality in Python.
"""

# The MNIST data is split between 60,000 28 x 28 pixel training images and 10,000 28 x 28 pixel images
(X_train, y_train), (X_test, y_test) = mnist.load_data()

print("X_train shape", X_train.shape)
print("y_train shape", y_train.shape)
print("X_test shape", X_test.shape)
print("y_test shape", y_test.shape)

"""Using Matplotlib, we can display some sample images from the training set directly on the screen."""

plt.rcParams['figure.figsize'] = (9,9) # Make the figures a bit bigger

for i in range(9):
    plt.subplot(3,3,i+1)
    num = random.randint(0, len(X_train))
    plt.imshow(X_train[num], cmap='gray', interpolation='none')
    plt.title("Class {}".format(y_train[num]))
    
plt.tight_layout()

"""Let's take a closer look at one digit and print out an array representing the last digit."""

# just a little function for pretty printing a matrix
def matprint(mat, fmt="g"):
    col_maxes = [max([len(("{:"+fmt+"}").format(x)) for x in col]) for col in mat.T]
    for x in mat:
        for i, y in enumerate(x):
            print(("{:"+str(col_maxes[i])+fmt+"}").format(y), end="  ")
        print("")

# now print!        
matprint(X_train[num])

"""Each pixel is an 8-bit integer from 0 to 255. 0 is completely black and 255 is completely white. This is what we call a single channel pixel. It's called monochrome.

*Fun fact! Your computer screen has three channels for each pixel: red, green, blue. Each of these channels also probably takes an 8-bit integer. 3 channels - 24 bits in total - 16,777,216 possible colors!*

## Formatting the input data layer

Instead of a 28 x 28 matrix, we build our network so that it takes a vector of length 784.

Each image then needs to be converted (or flattened) into a vector. We also normalize the input to be in the range [0-1] rather than [0-255].

<img src='https://github.com/AviatorMoser/keras-mnist-tutorial/blob/master/flatten.png?raw=1' >
"""

X_train = X_train.reshape(60000, 784) # reshape 60,000 28 x 28 matrices into 60,000 784-length vectors.
X_test = X_test.reshape(10000, 784)   # reshape 10,000 28 x 28 matrices into 10,000 784-length vectors.

X_train = X_train.astype('float32')   # change integers to 32-bit floating point numbers
X_test = X_test.astype('float32')

X_train /= 255                       # normalize each value for each pixel for the entire vector for each input
X_test /= 255 ##ЧИСЛО!

print("Training matrix shape", X_train.shape)
print("Testing matrix shape", X_test.shape)

"""We then modify our classes (unique digits) to be in

---

the same format, i.e.

```
0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0]
1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0]
2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0]
etc.
```

If the final output of our network is very close to one of these classes, then most likely it is this class. For example, if the final result is:

```
[0, 0.94, 0, 0, 0, 0, 0.06, 0, 0]
```
then it is most likely that this is an image of the number "1".
"""

nb_classes = 10 # number of unique digits

Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)

"""# Creation of a 3-layer fully connected neural network

<img src="https://github.com/AviatorMoser/keras-mnist-tutorial/blob/master/figure.png?raw=1" />
"""

# The Sequential model is a linear stack of layers and is very common.

model = Sequential()


# The first hidden layer is a set of 512 nodes (neurons).
# Each node will receive an element from each input vector and apply some weight and bias to it.

model.add(Dense(512, input_shape=(784,))) #(784,) is not a typo -- that represents a 784 length vector!

# "Activation" is a non-linear function applied to the output of the layer above.
# It checks the new value of the node and decides if this artificial neuron has fired.
# Rectified Linear Unit (ReLU) transforms all negative inputs into next level nodes so that they are zero.


model.add(Activation('relu'))

# Dropout resets the selection of random outputs (i.e. disables their activation)
# Dropout helps protect the model from remembering or "overfitting" the training data.
model.add(Dropout(0.2))


# The second hidden layer looks identical to our first layer.
# However, instead of each of the 512 nodes getting 784 inputs from the image input,
# they get 512 inputs from the output of the first 512-node layer.

model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.2))


# The last layer of 10 neurons is fully connected to the previous 512-node layer.
# The last FCN level must be equal to the number of classes desired (in this case 10).
model.add(Dense(10))

# Softmax activation is a probability distribution over K different possible outcomes.
# All its values are non-negative and add up to 1.

model.add(Activation('softmax'))

# Summarize the built model

model.summary()

"""## Compiling the model

Keras is built on top of Theano and TensorFlow. Both packages allow you to define a *computation graph* in Python, which is then compiled and run efficiently on the CPU or GPU without the overhead of a Python interpreter.

When compiling a model, Keras asks you to provide a **loss function** and an **optimizer**. The loss function we will be using here is called *categorical cross entropy* and is a loss function well suited for comparing two probability distributions.

Our predictions are probability distributions over ten different numbers (e.g. "we are 80% sure that this image is 3, 10% sure that this image is 3, 10% sure that this is 8, 5% is 2, etc.") and the goal is is a probability distribution. with 100% for the correct category and 0 for everything else. Cross entropy is a measure of how much your predicted distribution differs from your target distribution. [More on Wikipedia] (https://en.wikipedia.org/wiki/Cross_entropy)

The optimizer helps determine how fast a model learns using **gradient descent**. The rate at which you descend a gradient is called the **learning rate**.
"""

# Let's use the Adam optimizer for learning
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

"""## Train the model!
This is the fun part!

The size of the so-called batch determines how much data per step is used to compute the loss function, gradients, and backpropagation. Large batch sizes allow the network to complete training faster; However, there are other factors besides training speed that should be considered.

Too large a batch size smooths out the local minima of the loss function, forcing the optimizer to choose one because it thinks it has found the global minimum.

Too small a batch size creates a very noisy loss function, and the optimizer may never find the global minimum.

So it may take some trial and error to find the right batch size!
"""

EPOCHS = 5
model.fit(X_train, Y_train,
          batch_size=128, epochs=EPOCHS,
          verbose=1)

"""The two numbers, in order, represent the value of the network's loss function on the training set and the overall accuracy of the network on the training data. But how does it work with data it hasn't trained on?

## Evaluate Model's Accuracy on Test Data
"""

score = model.evaluate(X_test, Y_test)
print('Test score:', score[0])
print('Test accuracy:', score[1])

"""### Check output

It's always a good idea to check the result and make sure everything looks ok. Here we will look at a few examples when everything is correct, and a few examples when he is wrong.
"""

# The predict_classes function outputs the highest probability class
# according to the trained classifier for each input example.
# predicted_classes = model.predict_classes(X_test)
predict_x = model.predict(X_test) 
predicted_classes = np.argmax(predict_x,axis=1)

# Check which items we got right / wrong
correct_indices = np.nonzero(predicted_classes == y_test)[0]

incorrect_indices = np.nonzero(predicted_classes != y_test)[0]

plt.figure()
for i, correct in enumerate(correct_indices[:9]):
    plt.subplot(3,3,i+1)
    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')
    plt.title("Predicted {}, Class {}".format(predicted_classes[correct], y_test[correct]))
    
plt.tight_layout()
    
plt.figure()
for i, incorrect in enumerate(incorrect_indices[:9]):
    plt.subplot(3,3,i+1)
    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')
    plt.title("Predicted {}, Class {}".format(predicted_classes[incorrect], y_test[incorrect]))
    
plt.tight_layout()